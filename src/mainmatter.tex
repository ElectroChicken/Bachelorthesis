\chapter{Introduction}
\label{ch:Introduction}
Answer Set Programming (ASP) is one of the most successful programming paradigms for declarative problem solving in particular in the field of knowledge representation and reasoning. As such, it has found applications in many different fields both in academic and industry work~\cite{EGL16}.
However, even though it has been around for over 20 years~\cite{Nie99, MT98}, there is still a significant lack of good workflow tools. Tools like a testing suite, debugging tool or a annotation language can help make the development process more streamlined and allow different approaches that have proven very effective in conventional programming languages such as test driven development (TDD,~\cite{Fra+03}). Research into these topics exists and has yielded results, however many topics remain unexplored.

This thesis will focus on one of these mostly unexplored topics which is code coverage for ASP. Code coverage is a concept that is widely used in conventional programming languages. It's efficacy has been evaluated many times and it has proven to be one of the most practical ways to test the adequacy of existing test suites or to generate new test suites with high fault detection~\cite{GJG14}.
Unfortunately, the coverage concepts applied in conventional testing cannot be easily applied to answer set programs. Notions such as \emph{path coverage} or \emph{branch coverage} common in procedural programming languages rely on evaluating paths through the control flow graph of a program, but such a graph cannot be constructed for answer set programs since, due to their declarative nature, no explicit notion of execution exists.

Testing and coverage has been discussed for declarative programming. In~\cite{BJ98} \citeauthor{BJ98} introduce coverage notions for logic programs based on the Prolog model of posing queries to a program and resolving them using SLD resolution. The concepts of unification and anti-instances are used to define the coverage metrics. This means that even though Prolog is a declarative language and in many ways similar to ASP, these notions are not compatible with ASP as it relies on rule instantiation instead of unification.
Work has also been done on testing constraint programs, another paradigm used in logic programming, however it was concluded, that developing notions of test coverage would not be possible in this case~\cite{LGL10}.

Finally, while general approaches to testing in ASP have been discussed multiple times~\cite[examples:][]{GOT17, ABR21, Oet22}, the only attempt at realising code coverage for answer set programs has been done in the paper \citetitle{Jan+10} by~\textcite{Jan+10}. 
There, five coverage metrics are defined for propositional normal programs based on the existing concepts of path and branch coverage in conventional testing. The contributions of this work to this topic are twofold:
\begin{enumerate}
    \item Introduction of a syntactic transformation of answer set programs which allows the computation of the coverage metrics defined by \citeauthor{Jan+10} using the integrated ASP system clingo.
    \item Extension of the given definitions so that they can also apply to more complex program classes.
\end{enumerate}

\begin{comment}
- Answer Set Programming is of growing importance in both academic and industry work (source?) and clingo is a popular solver for this

- Workflow Tools that make working with such programs easier and more comfortable barely exist. Research into these topics is only 
now getting more traction (source?)

- Testing is a very important part of a conventional software design approach.(source?)

- The topic of code coverage and it's use for conventional programming languages has been shown (source?)

- With this work I want to build on the previous work done on code coverage in answer set programming in this paper by~\textcite{Jan+10}

- To this end I developed a way to efficiently implement the coverage metrics defined in the paper.

- My implementation allows me to compute coverage using answer set programming. It also extends the given metrics to function with
almost all existing language construct instead of just for propositional programs.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Preliminaries on answer set programming}
\label{ch:Preliminaries on answer set programming}
This chapter will provide a short introduction to ASP and lay down the basic definitions that will be used in the rest of the thesis. As mentioned in \Cref{ch:Introduction}, the coverage metrics are restricted to propositional normal programs. Hence we will focus on defining these type of programs instead of the larger class of ASP programs.


\section{Answer Set Programs}
\label{sec:Preliminaries on answer set programming/Answer Set Programs}

A normal logic program is a (finite) set of rules. These rules take the form
\[
    a \leftarrow b_1, \ldots , b_m, \text{not}\ c_1, \ldots , \text{not}\ c_n,
\]
where \(a, b_1, \ldots , b_m, c_1, \ldots ,c_n\) are propositional atoms with \(m,\ n \geq 0\) and "not" denotes \emph{negation-as-failure}. For such a rule, the \emph{head} of the rule $r$ is defined as \(H(r) = \{a\}\), the \emph{positive body} of $r$ \(B^+(r) = \{ b_1, \ldots , b_m\}\) and the \emph{negative body} of $r$ \(B^-(r) = \nolinebreak \{c_1, \ldots ,c_n\}\). Finally, the \emph{body} of $r$ is the Union of the positive body and the default negation of each atom in the negative body \(B(r) = B^+(r) \cup \{\text{not}\ c\ |\ c \in B^-(r)\}\). A rule $r$ is called a \emph{fact} if \(B(r) = \emptyset\) and $r$ is called a \emph{constraint} if \(H(r) = \emptyset\).
All atoms in a program come from the same fixed alphabet $A$.

An \emph{interpretation} $I$ is a finite set of atoms from the alphabet $A$. This interpretation \emph{satisfies} the body of a rule $r$, written as \(I \models B(r)\), if and only if \(B^+(r) \subseteq I\ \text{and}\ B^-(r) \cap I = \emptyset\). Given this definition one can then say that $I$ satisfies a rule $r$ or \(I \models r\), iff \(I \cap H(r) \neq \emptyset \ \text{whenever}\ I \models B(r)\). For a program $P$, an interpretation is a \emph{model} of $P$, \(I \models P\), iff for every rule \(r \in P,\ I \models r\) holds.
% examples? -> program a<-b. c<-not d., I={a,b,c} is a model of P
\begin{definition}
\label{def:reduct}
    The \emph{reduct} of a program $P$ relative to an interpretation $I$ is the program \(P^I = \{ H(r) \leftarrow B^+(r) \ | \ r \in P \ \text{and} \ B^-(r) \cap I = \emptyset\}\) \cite{GL88}
\end{definition}
An interpretation $I$ is called an \emph{answer set} of $P$, iff it is a minimal model of the reduct \(P^I\), i.e. \(I \models P^I \) and there exists no subset \( I' \subset I \) for which \( I' \models P^{I}\) holds. The collection of all answer sets of a program $P$ is denoted \(\text{AS}(P)\).
\begin{example}
\label{ex:reduct}
    Given the program $P$
    \begin{align*}
        a &\leftarrow \\
        c &\leftarrow a, b \\
        d &\leftarrow a,\ \text{not}\ b
    \end{align*}
    and the interpretation \(I = \{a, d\}\), the \emph{reduct} of $P$ relative to $I$ is \(P^I = \{a \leftarrow\ ; c \leftarrow a, b\ ; d \leftarrow a\}\). It can then be verified, that the set \(\{a,d\}\) is a minimal model of the reduct \(P^I\) as it satisfies every rule of the reduct. Therefore the interpretation $I$ is an \emph{answer set} of the program $P$. Indeed it is the only answer set of the program.
\end{example}
An atom $a$ is called a \emph{brave consequence} of the program $P$ if it is contained in at least one answer set of $P$, i.e. \(\exists X \in \text{AS}(P) \ \text{such that}\ a \in X\). The set of brave consequences of $P$ is \(\text{BC}(P) = \bigcup \text{AS}(P)\).
Respectively, an atom $a$ is called a \emph{cautious consequence} of $P$ if it is contained in all the answers sets of $P$, i.e. \(\forall X \in \text{AS}(P) \ \text{it holds that}\ a \in X\). The set of cautious consequences of $P$ is therefore \(\text{CC}(P) = \bigcap \text{AS}(P)\).

\begin{definition}
\label{def:herbrand universe}
    The \emph{herbrand universe} of a program $P$, \(\text{HU}_P\), is the set of all \emph{ground terms} that can be formed from all the constant symbols and function symbols in $P$.
\end{definition}

\begin{example}
\label{ex:herbrand universe}
    Let \(P = \{a \leftarrow\ ; b \leftarrow a\}\) then the set of constant symbols in $P$ is \(\{a,b\}\) and the set of function symbols is \(\emptyset\) as this is a propositional program. In this case \(\text{HU}_P = \{a,b\}\).
\end{example}

\begin{definition}
\label{def:herbrand base}
    The \emph{herbrand base} of a program $P$, \(\text{HB}_P\), is the set of all ground atomic formulas that can be formed from all the predicate symbols of $P$ and the terms in \(\text{HU}_P\).
\end{definition}

\begin{example}
\label{ex:herbrand base}
    For the program used in the previous example, there are no predicate symbols. Therefore \(\text{HB}_{P} = \text{HU}_{P} = \{a,b\}\). This is also true for any other propositional program. 
\end{example}

The \emph{definition} of an atom in a program $P$ is the set of defining rules for the atom \(a \in \text{HB}_P\), given by
\(
    \text{Def}_P(a) = \{r \in P \ | \ H(r) = \{a\}\}
\)
Furthermore, the set of \emph{supporting rules} of $P$ under an interpretation $I$ is defined as
\(
    \text{SuppR}(P, I) = \{r \in P \ | \ I \models B(r)\}
\)
\begin{example}
\label{ex:def/supp}
    Consider the program $P$
    \begin{align*}
        a &\leftarrow \\
        b &\leftarrow a,\ \text{not}\ c \\
        b &\leftarrow c,\ \text{not}\ a
    \end{align*}
    and the interpretation \(I = \{a, b\}\), then the set of defining rules of the atom b is \(\text{Def}_P(b) = \{b \leftarrow a,\ \text{not}\ c\ ; b \leftarrow c,\ \text{not}\ a\}\) and the set of supported rules under $I$ is \(\text{SuppR}(P, I) = \{a \leftarrow\ ; b \leftarrow a,\ \text{not}\ c\}\).
\end{example}

Finally we define the \emph{positive atom dependency graph} of a program $P$ as the directed graph \(G = (V, E)\) where the vertices are all the atoms occurring in the program \(V = \text{HB}_P\) and the edge \((a,b) \in E\) exists, iff there exists a rule \(r \in P\) such that \(a \in H(r) \ \text{and}\ b \in B^+(r)\) hold.
For such a graph, a \emph{loop} is a non empty set $L$ of atoms for which the subgraph of $G$ induced by $L$ is strongly connected. This means that for every pair of atoms \(a,b \in L\) there is a path $\pi$ in $G$ from $a$ to $b$ such that each atom in $\pi$ is in $L$.
A \emph{strongly connected component} (SCC) of a directed graph $G$ is a loop, i.e. it is strongly connected, that is maximal, meaning no additional vertices of $G$ can be added to the set without breaking its property of being strongly connected. Consequently any subset of a strongly connected component is also a loop.
A loop that contains exactly one atom is called singleton loop.
\begin{example}
\label{ex:dependency graph}
    Using the program given in \cref{ex:def/supp}, the positive atom dependency graph is defined as \(G = (V, E)\) with \(V = \{a, b, c\}\ \text{and}\ E = \{(b, a), (b, c)\}\). Therefore, the only loops this program contains are the singleton loops \(\{a\},\ \{b\},\ \text{and}\ \{c\}\). These are also all strongly connected components. 
    However, if the rules \(a \leftarrow b.\ \text{and}\ c \leftarrow a.\) are added to the program, the dependency graph of $P$ will now be \(G = (V, E)\ \text{with}\ V = \{a,b,c\}\ \text{and}\ E = \{(b,a), (b,c), (a,b), (c,a)\}\) (see \cref{fig:dependency graph}. In this new graph the set of all loops is \(L = \{\{a\},\{b\},\{c\},\{a,b\},\{a,b,c\}\}\). The set of SCCs only contains one element: \(C = \{\{a,b,c\}\}\).
\end{example}

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance={25mm}, thick, main/.style = {draw, circle}] 
        \node[main] (1) at (0,1) {a}; 
        \node[main] (2) at (2,1) {b}; 
        \node[main] (3) at (1,0) {c};  
        \draw[->] (1) edge [bend left=45] (2); 
        \draw[->] (2) -- (1); 
        \draw[->] (3) -- (1);
        \draw[->] (2) -- (3);
    \end{tikzpicture} 
    \caption{dependency graph of program in \cref{ex:dependency graph}}
    \label{fig:dependency graph}
\end{figure}


\begin{comment}
- Basic introduction to asp giving all the relevant definitions (with examples + sources (for all/some?)) 
    
    - rule, head, (positive/negative)body \/
    
    - interpretation that satisfies (positive/negative)body, rule, program \/

    - Def(a), SuppR(P,I) \/

    - answer sets, brave/cautious consequence \/

    - positive atom dependency graph, loops, strongly connected components  \/

-> adjust these so they work for variables etc. or do this later? (is this needed?/ is it a big adjustment?)
\end{comment}

\section{Testing in Answer Set Programming}
\label{sec:Preliminaries on answer set programming/Testing in Answer Set Programming}
In order to talk about testing answer set programs, some additional notions need to be established. Conventional testing generally consists of using a specific input to a program and observing whether the output of the program matches the expected output~\cite[71\psqq]{AO16}.

In ASP it is common practice that a problem is split in two separate programs: a \emph{problem instance} that contains the specific description of one instance of the problem class, represented as a set of facts and a \emph{uniform problem encoding} that describes the general problem class and its solutions. Thus when inputting a problem instance into the problem encoding it will find the corresponding solution and output it in form of answer sets that are usually filtered to dedicated output atoms. It is therefore natural to equate the problem instance to the conventional input and the answer sets encoding the solution to the conventional output. Based on this, we can define the \emph{input alphabet} as well as the \emph{output alphabet} of a program as a subsets of the alphabet $A$: \(\mathbb{I}_P \subseteq A\ \text{and}\ \mathbb{O}_P \subseteq A\), where the input alphabet contains all the atoms that may occur in a problem instance and the output alphabet contains the atoms relevant for the output.

In conventional software testing, a test case for a program $P$ always consists of an input $I$ of $P$ as well as the expected output of $P$ given $I$~\cite{MB11}. That way it can be verified, that a program produces the correct output given the input $I$.

A \emph{test suite} for $P$ is a collection of individual test cases and the \emph{exhaustive test suite} for $P$ is the suite \(\varepsilon_P\) that contains every possible test case for $P$, meaning that for every possible input \(I \subseteq \mathbb{I}_P\) there is a test case in the exhaustive test suite that consists of $I$ and the corresponding output, meaning there are a total of \(2^{|\mathbb{I}_P|}\) test cases in the exhaustive test suite and the inputs of $\varepsilon_P$ are the power set of the input alphabet \(inp(\varepsilon_P) = 2^{\mathbb{I}_P}\).

\begin{example}
\label{ex:test suite}
    Consider the program $P$
    \begin{align*}
        d &\leftarrow a,\ \text{not}\ b \\
        e &\leftarrow b, c \\
        f &\leftarrow a, b,\ \text{not}\ c \\
    \end{align*}
    Assuming that \(\mathbb{I}_P = \{a, b, c\}\) and \(\mathbb{O}_P = \{d, e, f\}\) a possible test case could be the pair \((I, O)\) where \(I = \{a, b\}\) and \(O = \{f\}\).
    The exhaustive test suite for this program would contain \(2^3 = 8\) different test cases: \(T_1 = (\emptyset, \emptyset)\), \(T_2 = (\{a\}, \{d\})\), \(T_3 
 = (\{b\}, \emptyset)\), \(T_4 = (\{c\}, \emptyset)\), \(T_5 = (\{a, b\}, \{f\})\), \(T_6 = (\{a, c\}, \{d\})\), \(T_7 = (\{b, c\}, \{e\})\), \(T_8 = (\{a, b, c\}, \{e\})\)
\end{example}

\begin{comment}
    
% - What is input and output of a program? /

- what is a testcase and a testsuite \/

- exhaustive test suite for P -> all possible testcases \/

- maybe Specification -> the correct (expected) output for every input, what does it mean for a program to "pass/be compliant 
with" a testcase, when is a program "correct" with respect to a specification (not actually needed for coverage as coverage 
does not care about specification!)    
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Coverage metrics}
\label{ch:Coverage metrics}


\begin{comment}
    - Analogous to concepts of coverage in other (conventional) programming languages (source) I introduce path-like and branch-like coverage metrics according to \textcite{Jan+10}
\end{comment}

\section{Coverage functions}
\label{sec:Coverage metrics/Coverage functions}


\begin{comment}
    - general definition of coverage functions (maybe additional source? / compare to what paper did)
    
    - talk about the difference between all objects and coverable objects
    
    - trivial/clairvoyant coverage functions maybe not important to discuss    
\end{comment}

\section{Path-like coverage}
\label{sec:Coverage metrics/Path-like coverage}


\begin{comment}
    - general introduction to path like coverage in conventional programming languages (source)
    
        - use the controlflow graph and cover every possible path through this graph
    
    - generally the most "complete" coverage metric
    
    -> these are generally very computationally expensive (exponentially many possible paths)
\end{comment}

\subsection{Program coverage}
\label{subsec:Coverage metrics/Path-like coverage/Program coverage}


\begin{comment}
    - Analogous to the conventional path-like coverage I define program coverage
    
    - Definition + example
    
    - show that total program coverage means all possible answer sets get produced by the testsuite
    
    - talk about the problems here? (complexity + not possible for programs with variables as it is necessary to enumerate all 
    possible inputs to find maximum coverage)
\end{comment}

\section{Branch-like coverage}
\label{sec:Coverage metrics/Branch-like coverage}


\begin{comment}
    - general introduction to branch-like coverage in conventional programming languages (source)
    
        - use the control flow graph and cover every possible branch through this graph
    
    - less complete but easier to compute, still very potent(?) (source)
    
    - there are different types of branch like coverage even in conventional programming languages (source), the same is true here
\end{comment}

\subsection{Rule coverage}
\label{subsec:Coverage metrics/Branch-like coverage/Rule coverage}


\begin{comment}
    - Definition + example
    
    - similar to program coverage in some ways but less complex!
    
    - some rules may sometimes (or always) not be coverable -> examples -> ties back to beginning of the chapter / thats why coverage 
    is defined on coverable objects
    
    (- total program coverage implies total rule coverage -> not so relevant but maybe interesting to mention?)
\end{comment}

\subsubsection{Constraint coverage}
\label{subsubsec:Coverage metrics/Branch-like coverage/Rule coverage/Constraint coverage}
(is this an extra section or should this be in the rule coverage section?)

- constraints are a special type of rule and have to be handled slightly differently because when the body of a constraint is true 
(=normal rule coverage) this will imply false and therefore not create an answer set / create an unsatisfiable solve call -> we 
cant check constraints the same way we check other rules

- solution: (following the suggestion in the paper by \textcite{Jan+11}) remove the constraint from the program in order to check 
for its covereage!

- Definition + examples

(- maybe reminder that these coverage metrics dont really care about what the output of the program is and whether its according to 
the specification, therefore removing constraints is okay even though it might completely destroy the functionality of the program)

\subsection{Loop coverage}
\label{subsec:Coverage metrics/Branch-like coverage/Loop coverage}
- loops play an important role in ASP as seen in (source). Therefore constructing a coverage metric that focuses on them makes sense

- Definition + example

- generally number of loops in a program in exponential in the number of rules -> expensive to compute! -> introduce 2 more 
coverage metrics that approximate loop coverage! (one for minimal (singleton) loops and one for maximal loops (strongyl connected 
components))

(- no real relation to rule coverage (neither implies the other)

- total program coverage implies total loop coverage -> not so relevant but maybe interesting to mention?)

\subsection{Definition coverage}
\label{subsec:Coverage metrics/Branch-like coverage/Definition coverage}
- 2 ways to introduce definition coverage: 
    
    - as a coverage metric for singleton loops (minimal loops) and therefore a special case of loop coverage

    - as a representation of the disjunctions in the program (if an atom "a" is defined in multiple rules you could rewrite 
    this as a if B1 v B2 v ...) -> this coverage metric covers these implicit disjunctions
-> discuss both but in which order?

- Definition + example

(- this is different to rule coverage! -> total positive rule coverage implies total positive definition coverage, not the other 
way around and no connection for negative coverage!

- total loop coverage implies total definition coverage

- total program coverage implies total definition coverage)

\subsection{Component coverage}
\label{subsec:Coverage cetrics/Branch-like coverage/Component coverage}
- strongly connected components are the maximal loops of the program, therefore this is an approximation of loop coverage

- Definition + example

- Definition of negative coverage is different from loop coverage!

- because of this different definition positive/negative component coverage of a specific component implies positive/negative loop 
coverage for all subset loops of the component (would not be the case for negative coverage if definition is different)

(- total positive loop coverage implies total positive component coverage, however this is not true for negative coverage because 
of the different definition (see above)

- total program coverage implies total component coverage)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Computing coverage metrics for propositional programs}
\label{ch:Computing coverage metrics for propositional programs}
- one of the main contributions of this thesis is finding a way to build a prototype tool for checking coverage of ASP programs 
based on the theoretic definitions of coverage described in the previous chapter.

- as the definitions are only applicable to propositional programs, those will be my focus for now. However such a tool would not 
be very practical so ideally an approach should be chosen that can easily be extended.

(maybe include a short history of what i tried but didnt work? -> meta programming with reify output)

\section{General approach}
\label{sec:Computing coverage metrics for propositional programs/General approach}
- the goal is to compute the coverage using ASP

-> introduce labels for each coverage metric -> add them to the program in a specific way (based on the coverage metric)  -> solve 
normally using clingo -> if the label is in the answer set, the corresponding object is covered

- I only add new rules, dont change or take away existing ones (except constraints -> see above) and these new rules only produce 
labels not predicates that are part of the original program -> the resulting program is still equivalent! (maybe proof?)

- with this method, to find total positive and negative coverage it is not necessary to look at every model in the solve call. 
Instead use one solve call with brave consequences and one with cautious consequences! -> more efficient than looking at every model!

- labels can also be used to add the testcases to the program -> this way coverage for all testcases can be computed in the same solve 
call instead of needing one call per testcase

-> this is done by: adding a choice rule $\{\_i0;...;\_in\}$. for n testcases in the testsuite, adding rules $a_i :- i_j$. for every atom i 
in the testcase j.
(- why does this work?
- why does this not change the program?)

- maximum coverage needs to be computed to get accurate coverage numbers!

-> done by calculating the coverage normally but instead of testcases adding a rule $\{a_0;...;a_n\}$. where $a_0...a_n$ are all possible 
inputatoms (see definition input/output) -> these have to be specified in advance by the user!

\subsection{Rule coverage}
\label{subsec:Computing coverage metrics for propositional programs/General approach/Rule coverage}
- $\_ri$ label for every rule i

- add new rule to program for each label: $\{\_ri\}$ is the head, the body is identical to the body of the rule i

- example

- if the body of rule i is true in some answer set (aka $X |= B(r_i)$) then rule i is positively covered and $\_ri$ will appear in the answer set 

-> $\_ri$ in answer set <=> rule i is positively covered

-> $\_ri$ not in answer set <=> rule i is negatively covered (proof for these?)

\subsection{Definition coverage}
\label{subsec:Computing coverage metrics for propositional programs/General approach/Definition coverage}
- $\_di$ label for every atom i

- if atom i is definable (it appears in the head of a rule, aka $Def(a_i)\neq\emptyset$) add new rule to program: $\_di :- \_rj$. for every rule j that defines atom i 
(every rule j that has atom i in its head, aka every $r_j \in Def(a_i)$)

- example

- if one of the rules is covered (its body is true) then atom i is covered and $\_di$ will appear in the answer set

-> $\_di$ in answer set <=> atom i is positively covered

-> $\_di$ not in answer set <=> atom i is negatively covered (proof for these?)

\subsection{Loop coverage}
\label{subsec:Computing coverage metrics for propositional programs/General approach/Loop coverage}
- $\_li$ label for every loop i

- first need to find all the loops in the program! -> build positive atom dependency graph, find sccs and then find subsets of sccs 
that are loops

- for each loop i that consists of atoms $a_m$ to $a_n$ add new rule to program: $\_li :- \_dm, ..., \_dn$.

- if all the atoms $a_m$ to $a_n$ that constitute the loop i are defined (aka definition covered), then all the $\_dm$ to $\_dn$ are true, 
then loop i is covered and $\_li$ will be in the answer set.

-> $\_li$ in answer set <=> loop i is positively covered

- if any of the atoms $a_m$ to $a_n$ are not defined the loop is negatively covered and  $\_li$ will not be in the answer set

-> $\_li$ not in answer set <=> loop i is negatively covered

\subsection{Component coverage}
\label{subsec:Computing coverage metrics for propositional programs/General approach/Component coverage}
- $\_si$ label for every strongly connected component i

- find sccs same way as loops

- construct new rules same way as for loops: $\_si :- \_dm, ... , \_dn$.

-> $\_si$ in answer set <=> scc i is positively covered

- HOWEVER! $\_si$ not in answer set -/> scc i is negatively covered! (see definition of component coverage)

-> add additional rules to program: $\_nsi :- not \_dm, ..., not \_dn$.

- if NONE of the atoms $a_m$ to $a_n$ are defined (aka definition covered), scc i is negatively covered and $\_nsi$ will be in the anser set

-> $\_nsi$ in answer set <=> scc i is negatively covered

\subsection{Program coverage}
\label{subsec:Computing coverage metrics for propositional programs/General approach/Program coverage}
- use the $\_ri$ labels from rule coverage, no new labels needed!

- a subprogram $P' \subseteq P$ is covered if exactly all rules contained in $P'$ are covered and no other rules are covered

-> each answer set covers exactly one subprogram -> it is necessary to look at every answer set instead of just brave/cautious like 
with the other coverage metrics -> has to be computed seperately from the other coverage metrics!

- for $P = \{r_1,...,r_n\}$,  $\{\_rx,...,\_ry\}$ are the rule labels in an answer set <=> $P'=\{r_x,...,r_y\}$ is covered

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Coverage for further program classes}
\label{ch:Coverage for further program classes}
- the given definitions of the coverage metrics are only for propositional programs but this is not very practical as most programs 
are more complex than that. They contain many complex language constructs supported by ASP/clingo

-> to make this coverage check actually usable these metrics have to be extended to work for all these constructs

- in general, the label approach should allow me to easily apply these coverage metrics to further program classes with very little 
adjustments!

\section{Adjusted definitions}
\label{sec:Coverage for further program classes/Adjusted definitions}
- starting with basic definitions, which have to be changed? -> list here

- then go through coverage metric definitions -> if they dont have to be changed, why?

\section{Language construct}
\label{sec:Coverage for further program classes/Language constructs}
- go through all the additional constructs one by one, explain how they should work, why they do work like that or not?
-> table

- Big difference: maximal coverage can not be computed as this requires listing all possible inputs. This is not possible with variables 
as there can be infinitely many -> give coverage as covered/total existing (!!! is this even a correct coverage function???)
(!!! possible extension: allow user to specify domain for each variable -> if domain is not infinite then computing max cov is possible !!!) 

-> thats all the difference!?!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{ch:Conclusion}
(Zusammenfassung)
- I managed to implement the coverage metrics defined by \textcite{Jan+10} in a way that makes it possible to compute the coverage 
of an entire testsuite with the help of just 2 clingo solve calls

- I also extended the coverage metrics to cover almost all existing language constructs in ASP/clingo, making them more "real world" 
applicable


(Outlook)
- As mentioned in the previous section, theory atoms and constraint terms are currently not considered when calculating definition, loop 
and component coverage -> should be an easy extension!

- rebuilding the app to work with the ClingoApp could make working with external atoms, constants and different program parts possible 
or easier (by allowing interaction with the clingo solver through the command line)

- in this work, complexity and efficiency are not priorities, therefore the current program is not very optimized!

- for this prototype I tried to take the existing definitions of coverage in ASP and extend them to more complex programs with as little 
changes to the definitions as possible. This might not be the best way! Some of the definitions might need more changes:

    - Definition coverage and choice rules: if an atom is only defined in a choice rule (ex. $\{a\}.$) it will only be considered positively 
    covered, not negatively covered, if the body is true, even though the atom will be false in some answer sets (therefore acting as if 
    it was negatively covered -> the "path" where the atom is negatively covered is executed, but the atom is not considered negatively covered)

    - loop/component coverage: a loop is considered covered, if all atoms contained in the loop are definition covered. Due to the nature 
    of definition coverage it is however possible for an atom to be covered at multiple places! -> it is thus possible to cover all 
    atoms in a loop without ever "executing" the loop -> this can lead to problems that are caused by a loop not being discovered by 
    a testcase, even though that testcase has total loop coverage! (example!)

- simply checking coverage for a given testsuite is only one use case for coverage metrics! They can also be used to automatically generate 
testcases that are meant to catch a maximum amount of errors. The idea is explored in \cite{Jan+11}. This can certainly also be done with 
my implementation.

- these coverage metrics have not really been tested! In the paper \cite{Jan+11} only rule and definition coverage have been tested for 
their practicality in a "real world" scenario. It is unknown how effective loop and component coverage are. -> This needs testing! 
During these tests potential changes to the definitions could also be evaluated.

- also in the line of testing the coverage metrics it would be interesting to see how well definition and component coverage approximate 
loop coverage and whether program coverage does actually give the best results given that it is the most "complete" metric
-> maybe figure out a guideline on which metrics to use when. The current setup where mixing any metrics is possible does not make much 
sense (as for example definition coverage is fully contained in loop coverage)!


(Fazit)
- in this thesis I laid the ground work to one day implement coverage checks and maybe coverage-based testgeneration in a full unit 
testing api for ASP programs

- the simple nature of my approach should make it easy to extend to program with new or improved coverage metrics and implement it 
into a larger testing framework

- this is working towards making Answer Set Programming more accessible and more efficient by providing tools to support the 
developpement process


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Test}
\label{ch:Test}

\begin{enumerate}
    \item Bla.\\
    (Bla.)
    \item Bla. (Bla.)
\end{enumerate}

\Cref{ch:Introduction} Referenz zu Kapitel.
\Cref{sec:Test/Perzepte_und_Symbole} Referenz zu Unterkapitel. 
\Cref{fig:cups_yolo} Referenz zu Bild.
\Cref{lst:cups_symbolic} Referenz zu Listing
\textcite{Jan+10} Zitat mit Namen der Autoren
\cite{Jan+10} Zitat nur mit Abkürzung
\ac{ASP} Link zu Abkürzungen
\emph{symbol grounding problem} 
\marginpar{Symbol} Randkommentar
\footnote{Bla} Fussnote

\section{Perzepte und Symbole}
\label{sec:Test/Perzepte_und_Symbole}
Bla.

%\begin{definition}
%    \label{def:Perzept}
%    Ein \emph{Perzept}\marginpar{Perzept} ist der sensorische Eindruck eines physikalischen Objektes zu einem bestimmten Zeitpunkt.
%\end{definition}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{gfx/unilogo.jpg}
    \caption{Ein Kamerabild mit eingezeichneten Perzepten.}
    \label{fig:cups_yolo}
\end{figure}


\begin{lstlisting}[float,caption={Eine symbolische Beschreibung der Objekte in bla.},label=lst:cups_symbolic]
symbol(cup_1; cup_2; cup_3; spoon; diningtable).

is_on(
    cup_1, diningtable;
    cup_2, diningtable;
    cup_3, diningtable
).

is_inside_of(spoon, cup_3).

contains(
    cup_1, coffee;
    cup_2, coffee;
    cup_3, hot_chocolate
).

\end{lstlisting}



%\begin{figure}
%    \centering
%    \begin{tikzpicture}[
%        ->,
%        >={Stealth[round]},
%        align=center,
%        state/.style={
%            draw,
%            rectangle,
%            rounded corners=3mm
%        },
%        every edge/.append style={thick}
%    ]
%        \node (A) [state]             {Symbol verankert};
%        \node (B) [state, below=of A] {Symbol nicht verankert};
%
%        \node (1) [left =25mm of A, font=\scriptsize] {ausgehend von\\einem Perzept};
%        \node (2) [right=25mm of A, font=\scriptsize] {ausgehend von\\einem Symbol};
%        \node (3) [below=of B,      font=\scriptsize] {Symbol gelöscht};
%
%        \path (A) edge [loop above] node [above] {\emph{Verfolgen}}    (A);
%        \path (A) edge [bend left]  node [right] {\emph{Verlieren}}    (B);
%        \path (B) edge [bend left]  node [left]  {\emph{Wiederfinden}} (A);
%        \path (1) edge              node [above] {\emph{Entdecken}}    (A);
%        \path (2) edge              node [above] {\emph{Finden}}       (A);
%        \path (B) edge              node [right] {\emph{Zerstören}}    (3);
%    \end{tikzpicture}
%    \caption{Die Verankerungsfunktionen als Zustandsübergänge, frei nach \cite[Abbildung~4]{Gün+18}.}
%    \label{fig:anchoring_functions_as_state_transitions}
%\end{figure}




$$\operatorname{match}(\sigma, \gamma) \Leftrightarrow \forall p \in \sigma \; \exists \phi \in \operatorname{feat}(\gamma): \; g(p, \phi, \gamma(\phi))$$


\begin{table}
    \centering
        \begin{tabularx}{0.5\textwidth}{l|X|l}
            $M$             & $P_1^M$                               & $Cn(P_1^M)$  \\
            \hline
            $\emptyset$     & $\{ a \leftarrow a, b \leftarrow \}$  & $\{ b \}$     \\
            $\{ a \}$       & $\{ a \leftarrow a \}$                & $\emptyset$   \\
            $\{ b \}$       & $\{ a \leftarrow a, b \leftarrow \}$  & $\{ b \}$     \\
            $\{ a, b \}$    & $\{ a \leftarrow a \}$                & $\emptyset$   \\
        \end{tabularx}
    \caption[$P_1 = \{ a \leftarrow a, b \leftarrow naf a \}$ hat ein stabiles Modell.]{$P_1 = \{ a \leftarrow a, b \leftarrow naf a \}$ hat ein stabiles Modell $\{ b \}$.}
    \label{tab:Ein_stabiles_Modell}
\end{table}

%\begin{example}
%    Das Programm $P = \{ \; \{ a, b\} \; \}$ hat vier stabile Modelle, nämlich die Elemente von $2^{\{a, b\}}$.
%\end{example}

%\begin{example}
%   Das Programm
%    $$
%        P =
%        \begin{Bmatrix}
%            \operatorname{cup}(1) \\
%            \operatorname{cup}(2) \\
%            1~\{~\operatorname{blue}(X) : \operatorname{cup}(X)~\}~1 \\
%        \end{Bmatrix}
%    $$
%    hat die Grundinstanz
%    $$
%        \grd(P) =
%        \begin{Bmatrix}
%            \operatorname{cup}(1) \\
%            \operatorname{cup}(2) \\
%            1~\{~\operatorname{blue}(1),~\operatorname{blue}(2)~\}~1 \\
%        \end{Bmatrix}
%    $$
%    und die stabilen Modelle $\{\operatorname{cup}(1),{ }\operatorname{cup}(2),{ }\operatorname{blue}(1)\}$ und \linebreak$\{\operatorname{cup}(1),{ }\operatorname{cup}(2),{ }\operatorname{blue}(2)\}$.
%\end{example}

\begin{proof}
    Zu jeder Teilmenge $M \subseteq A = \{ a, b, c \}$ ist $P^M = P$.
    Die Teilmengen~$\emptyset$, $\{ a \}$, $\{ c \}$, $\{ a, b \}$ und $\{ b, c \}$ sind keine Modelle von $P^M$. $\{ a, c \}$, $\{ a, b, c\}$ und $\{ b \}$ sind Modelle von $P^M$.
    $\{ a, b, c\}$ ist kein minimales Modell von $P^M$, da $\{ b \} \subseteq \{ a, b, c\}$.
    Da $\{ a, c \} \nsubseteq \{ b \}$ und $\{ b \} \nsubseteq \{ a, c \}$, sind beide Modelle minimal und damit stabile Modelle von $P$.
\end{proof}

\code{\#show p(X,Y) : q(X).}

Test für \code{\#show p(X)} in einer Zeile.
%\lstinputlisting[float,caption={[Ein Graph mit 6 Knoten und 17 Kanten\\(\code{graph.lp}).]Ein Graph mit 6 Knoten und 17 Kanten (\code{graph.lp}).},label=lst:graphcolor/graph.lp]{../../code/graphcolor/graph.lp}

\begin{align*}
    &X                          &=\ &\{ \text{cup}_1, \text{cup}_2 \} \\
    &\Pi                        &=\ &\{ \pi_1, \pi_2, \pi_3 \} \\
    &\Phi                       &=\ &\{ \text{coffee}, \text{tea}, \text{hot}, \text{cold} \} \\
    &T                          &=\ &\{ t_1, t_2 \} \\
    &\beta(\text{cup}_1, t_1)   &=\ &\{ \text{coffee} \} \\
    &\beta(\text{cup}_2, t_1)   &=\ &\emptyset \\
    &\beta(\pi_1, t_1)          &=\ &\{ \text{coffee} \} \\
    &\beta(\pi_2, t_1)          &=\ &\{ \text{tea}, \text{cold} \} \\
    &\beta(\pi_3, t_2)          &=\ &\{ \text{tea} \}
\end{align*}


\cleardoublepage

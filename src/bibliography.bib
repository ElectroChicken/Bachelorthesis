@incollection{Jan+10,
  title         = {On testing answer-set programs},
  author        = {Janhunen, Tomi and Niemelä, Ilkka and Oetsch, Johannes and Pührer, Jörg and Tompits, Hans},
  booktitle     = {ECAI 2010},
  pages         = {951--956},
  year          = {2010},
  publisher     = {IOS Press}
} %%% main paper

@InProceedings{Jan+11,
  author        = {Janhunen, Tomi and Niemelä, Ilkka and Oetsch, Johannes and Pührer, Jörg and Tompits, Hans},
  editor        = {Delgrande, James P. and Faber, Wolfgang},
  title         = {Random vs. Structure-Based Testing of Answer-Set Programs: An Experimental Comparison},
  booktitle     = {Logic Programming and Nonmonotonic Reasoning},
  year          = {2011},
  publisher     = {Springer Berlin Heidelberg},
  address       = {Berlin, Heidelberg},
  pages         = {242--247},
  abstract      = {Answer-set programming (ASP) is an established paradigm for declarative problem solving, yet comparably little work on testing of answer-set programs has been done so far. In a recent paper, foundations for structure-based testing of answer-set programs building on a number of coverage notions have been proposed. In this paper, we develop a framework for testing answer-set programs based on this work and study how good the structure-based approach to test input generation is compared to random test input generation. The results indicate that random testing is quite ineffective for some benchmarks, while structure-based techniques catch faults with a high rate more consistently also in these cases.},
  isbn          = {978-3-642-20895-9}
} %%% automatic test generation

@article{EGL16,
  title={Applications of answer set programming},
  author={Erdem, Esra and Gelfond, Michael and Leone, Nicola},
  journal={AI Magazine},
  volume={37},
  number={3},
  pages={53--68},
  year={2016}
} %%% Applications

@article{MT98,
  doi = {10.48550/ARXIV.CS/9809032},  
  url = {https://arxiv.org/abs/cs/9809032},  
  author = {Marek, Victor W. and Truszczynski, Miroslaw},
  keywords = {Logic in Computer Science (cs.LO), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.3, I.2.4},
  title = {Stable models and an alternative logic programming paradigm},
  publisher = {arXiv},
  year = {1998},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
} %%% Origins of ASP

@article{Nie99,
  title={Logic programs with stable model semantics as a constraint programming paradigm},
  author={Niemel{\"a}, Ilkka},
  journal={Annals of mathematics and Artificial Intelligence},
  volume={25},
  number={3},
  pages={241--273},
  year={1999},
  publisher={Springer}
} %%% Origins of ASP

@InProceedings{Fra+03,
  author={Fraser, Steven and Beck, Kent and Caputo, Bill and Mackinnon, Tim and Newkirk, James and Poole, Charlie},
  editor={Marchesi, Michele and Succi, Giancarlo},
  title={Test Driven Development (TDD)},
  booktitle={Extreme Programming and Agile Processes in Software Engineering},
  year={2003},
  publisher={Springer Berlin Heidelberg},
  address={Berlin, Heidelberg},
  pages={459--462},
  abstract={This panel brings together practitioners with extensive experience in agile/XP methodologies to discuss the approaches and benefits of applying TDD. The goal of test driven development (TDD) is clean code that works. The mantra of TDD is: write a test; make it run; and make it right. Open questions exist, for example --- how can TDD approaches be applied to databases, GUIs, and distributed systems? What are the quantitative benchmarks that can demonstrate the value of TDD, and what are the best approaches to solve the ubiquitous issues of scalability?},
  isbn={978-3-540-44870-9},
  doi={10.1007/3-540-44870-5_84}
} %%% Test Driven Development

@InProceedings{GOT17,
  author        = {Gre{\ss}ler, Alexander and Oetsch, Johannes and Tompits, Hans},
  editor        = {Balduccini, Marcello and Janhunen, Tomi},
  title         = {Harvey: A System for Random Testing in ASP},
  booktitle     = {Logic Programming and Nonmonotonic Reasoning},
  year          = {2017},
  publisher     = {Springer International Publishing},
  address       = {Cham},
  pages         = {229--235},
  abstract      = {We present {\$}{\$}{\backslash}mathsf {\{}Harvey{\}}{\$}{\$}, a tool for random testing in answer-set programming (ASP) that allows to incorporate constraints to guide the generation of test inputs. Due to the declarative nature of ASP, it can be argued that there is less need for testing than in conventional software development. However, it is shown in practice that testing is still needed when more sophisticated methods are not viable. Random testing is recognised as a simple yet effective method in this regard. The approach described in this paper allows for random testing of answer-set programs in which both test-input generation and determining test verdicts is facilitated using ASP itself: The test-input space is defined using ASP rules and uniformity of test-input selection is achieved by using XOR sampling. This allows to go beyond simple random testing by adding further ASP constraints in the process.},
  isbn          = {978-3-319-61660-5},
  doi           = {10.1007/978-3-319-61660-5_21}
} %%% Testing ASP


@InProceedings{ABR21,
  author        = {Amendola, Giovanni and Berei, Tobias and Ricca, Francesco},
  editor        = {Faber, Wolfgang and Friedrich, Gerhard and Gebser, Martin and Morak, Michael},
  title         = {Testing in ASP: Revisited Language and Programming Environment},
  booktitle     = {Logics in Artificial Intelligence},
  year          = {2021},
  publisher     = {Springer International Publishing},
  address       = {Cham},
  pages         = {362--376},
  abstract      = {Unit testing frameworks are nowadays considered a best practice, foregone in almost all modern software development processes, to achieve rapid development of correct specifications. The first unit testing specification language for Answer Set Programming (ASP) was proposed in 2011 as a feature of the ASPIDE development environment. Later, a more portable unit testing language was included in the LANA annotation language. In this paper we propose a revisited unit testing specification language that allows one to inline tests within ASP program and an ASP-based test execution mechanism. Moreover, we present a programming environment supporting test driven development (TDD) of ASP programs with our language.},
  isbn          = {978-3-030-75775-5},
  doi           = {10.1007/978-3-030-75775-5_24}
} %%% Testing ASP

@phdthesis{Oet22,
  title={Testing for ASP-ASP for Testing},
  author={Oetsch, Johannes},
  year={2022},
  school={Wien},
  doi={10.34726/hss.2022.102508}
} %%% Testing ASP

@inproceedings{GJG14,
  author={Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
  title={Code Coverage for Suite Evaluation by Developers},
  year={2014},
  isbn={9781450327565},
  publisher={Association for Computing Machinery},
  address={New York, NY, USA},
  url={https://doi.org/10.1145/2568225.2568278},
  doi={10.1145/2568225.2568278},
  abstract={One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best.},
  booktitle={Proceedings of the 36th International Conference on Software Engineering},
  pages={72–82},
  numpages={11},
  keywords={statistical analysis, test frameworks, evaluation of coverage criteria},
  location={Hyderabad, India},
  series={ICSE 2014}
} %%% Effectiveness of code coverage

@INPROCEEDINGS{KTL15,
  author={Kochhar, Pavneet Singh and Thung, Ferdian and Lo, David},
  booktitle={2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)}, 
  title={Code coverage and test suite effectiveness: Empirical study with real bugs in large systems}, 
  year={2015},
  volume={},
  number={},
  pages={560-564},
  doi={10.1109/SANER.2015.7081877}
} %%% Effectiveness of code coverage

@INPROCEEDINGS{Hem15,
  author        = {Hemmati, Hadi},
  booktitle     = {2015 IEEE International Conference on Software Quality, Reliability and Security}, 
  title         = {How Effective Are Code Coverage Criteria?}, 
  year          = {2015},
  volume        = {},
  number        = {},
  pages         = {151-156},
  doi           = {10.1109/QRS.2015.30}
} %%% Effectiveness of code coverage

@article{BJ98,
  author    = {Fevzi Belli and Oliver Jack},
  title     = {Declarative Paradigm of Test Coverage},
  journal   = {Softw. Test. Verification Reliab.},
  volume    = {8},
  number    = {1},
  pages     = {15--47},
  year      = {1998},
  url       = {https://doi.org/10.1002/(SICI)1099-1689(199803)8:1\<15::AID-STVR146\>3.0.CO;2-D},
  doi       = {10.1002/(SICI)1099-1689(199803)8:1\<15::AID-STVR146\>3.0.CO;2-D},
  timestamp = {Wed, 01 Apr 2020 08:46:21 +0200},
  biburl    = {https://dblp.org/rec/journals/stvr/BelliJ98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
} %%% Code coverage in Prolog/declarative programming

@InProceedings{LGL10,
  author={Lazaar, Nadjib and Gotlieb, Arnaud and Lebbah, Yahia},
  editor={Cohen, David},
  title={On Testing Constraint Programs},
  booktitle={Principles and Practice of Constraint Programming -- CP 2010},
  year={2010},
  publisher={Springer Berlin Heidelberg},
  address={Berlin, Heidelberg},
  pages={330--344},
  abstract={The success of several constraint-based modeling languages such as OPL, ZINC, or COMET, appeals for better software engineering practices, particularly in the testing phase. This paper introduces a testing framework enabling automated test case generation for constraint programming. We propose a general framework of constraint program development which supposes that a first declarative and simple constraint model is available from the problem specifications analysis. Then, this model is refined using classical techniques such as constraint reformulation, surrogate and global constraint addition, or symmetry-breaking to form an improved constraint model that must be thoroughly tested before being used to address real-sized problems. We think that most of the faults are introduced in this refinement step and propose a process which takes the first declarative model as an oracle for detecting non-conformities. We derive practical test purposes from this process to generate automatically test data that exhibit non-conformities. We implemented this approach in a new tool called CPTEST that was used to automatically detect non-conformities on two classical benchmark programs, namely the Golomb rulers and the car-sequencing problem.},
  isbn={978-3-642-15396-9},
  doi={10.1007/978-3-642-15396-9_28}
} %%% Testing constraint programs


@article{TH02,
  author        = {Tikir, Mustafa M. and Hollingsworth, Jeffrey K.},
  title         = {Efficient Instrumentation for Code Coverage Testing},
  year          = {2002},
  issue_date    = {July 2002},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  volume        = {27},
  number        = {4},
  issn          = {0163-5948},
  url           = {https://doi.org/10.1145/566171.566186},
  doi           = {10.1145/566171.566186},
  abstract      = {Evaluation of Code Coverage is the problem of identifying the parts of a program that did not execute in one or more runs of a program. The traditional approach for code coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of code coverage. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38-90\% compared with purecov, a commercial code coverage tool. Our tool is fully automated and available for download from the Internet.},
  journal       = {SIGSOFT Softw. Eng. Notes},
  month         = {jul},
  pages         = {86-96},
  numpages      = {11},
  keywords      = {dynamic code patching, testing, code coverage, on-demand instrumentation, dominator tree, dynamic code deletion}
}
  

@article{WDC10,
  title         = {A family of code coverage-based heuristics for effective fault localization},
  journal       = {Journal of Systems and Software},
  volume        = {83},
  number        = {2},
  pages         = {188-208},
  year          = {2010},
  note          = {Computer Software and Applications},
  issn          = {0164-1212},
  doi           = {https://doi.org/10.1016/j.jss.2009.09.037},
  url           = {https://www.sciencedirect.com/science/article/pii/S0164121209002465},
  author        = {W. {Eric Wong} and Vidroha Debroy and Byoungju Choi},
  keywords      = {Fault localization, Program debugging, Code coverage, Heuristics, Suspiciousness of code, Successful tests, Failed tests},
  abstract      = {Locating faults in a program can be very time-consuming and arduous, and therefore, there is an increased demand for automated techniques that can assist in the fault localization process. In this paper a code coverage-based method with a family of heuristics is proposed in order to prioritize suspicious code according to its likelihood of containing program bugs. Highly suspicious code (i.e., code that is more likely to contain a bug) should be examined before code that is relatively less suspicious; and in this manner programmers can identify and repair faulty code more efficiently and effectively. We also address two important issues: first, how can each additional failed test case aid in locating program faults; and second, how can each additional successful test case help in locating program faults. We propose that with respect to a piece of code, the contribution of the first failed test case that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second failed test case that executes it, which in turn is larger than or equal to that of the third failed test case that executes it, and so on. This principle is also applied to the contribution provided by successful test cases that execute the piece of code. A tool, χDebug, was implemented to automate the computation of the suspiciousness of the code and the subsequent prioritization of suspicious code for locating program faults. To validate our method case studies were performed on six sets of programs: Siemens suite, Unix suite, space, grep, gzip, and make. Data collected from the studies are supportive of the above claim and also suggest Heuristics III(a), (b) and (c) of our method can effectively reduce the effort spent on fault localization.}
}


@inproceedings{Oet+12,
  author        = {Oetsch, Johannes and Prischink, Michael and P\"{u}hrer, J\"{o}rg and Schwengerer, Martin and Tompits, Hans},
  title         = {On the Small-Scope Hypothesis for Testing Answer-Set Programs},
  year          = {2012},
  isbn          = {9781577355601},
  publisher     = {AAAI Press},
  abstract      = {In software testing, the small-scope hypothesis states that a high proportion of errors can be found by testing a program for all test inputs within some small scope. In this paper, we evaluate the small-scope hypothesis for answer-set programming (ASP). To this end, we follow work in traditional testing and base our evaluation on mutation analysis. In fact, we show that a rather limited scope is sufficient for testing ASP encodings from a representative set of benchmark problems. Our experimental evaluation facilitates effective methods for testing in ASP. Also, it gives some justification to analyse programs at the propositional level after grounding them over a small domain.},
  booktitle     = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
  pages         = {43–53},
  numpages      = {11},
  location      = {Rome, Italy},
  series        = {KR'12}
}

@incollection{Jan+10,
  title         = {On testing answer-set programs},
  author        = {Janhunen, Tomi and Niemelä, Ilkka and Oetsch, Johannes and Pührer, Jörg and Tompits, Hans},
  booktitle     = {ECAI 2010},
  pages         = {951--956},
  year          = {2010},
  publisher     = {IOS Press}
}

@InProceedings{Jan+11,
  author        = {Janhunen, Tomi and Niemelä, Ilkka and Oetsch, Johannes and Pührer, Jörg and Tompits, Hans},
  editor        = {Delgrande, James P. and Faber, Wolfgang},
  title         = {Random vs. Structure-Based Testing of Answer-Set Programs: An Experimental Comparison},
  booktitle     = {Logic Programming and Nonmonotonic Reasoning},
  year          = {2011},
  publisher     = {Springer Berlin Heidelberg},
  address       = {Berlin, Heidelberg},
  pages         = {242--247},
  abstract      = {Answer-set programming (ASP) is an established paradigm for declarative problem solving, yet comparably little work on testing of answer-set programs has been done so far. In a recent paper, foundations for structure-based testing of answer-set programs building on a number of coverage notions have been proposed. In this paper, we develop a framework for testing answer-set programs based on this work and study how good the structure-based approach to test input generation is compared to random test input generation. The results indicate that random testing is quite ineffective for some benchmarks, while structure-based techniques catch faults with a high rate more consistently also in these cases.},
  isbn          = {978-3-642-20895-9}
}

@INPROCEEDINGS{Hem15,
  author        = {Hemmati, Hadi},
  booktitle     = {2015 IEEE International Conference on Software Quality, Reliability and Security}, 
  title         = {How Effective Are Code Coverage Criteria?}, 
  year          = {2015},
  volume        = {},
  number        = {},
  pages         = {151-156},
  doi           = {10.1109/QRS.2015.30}
}

@article{TH02,
  author        = {Tikir, Mustafa M. and Hollingsworth, Jeffrey K.},
  title         = {Efficient Instrumentation for Code Coverage Testing},
  year          = {2002},
  issue_date    = {July 2002},
  publisher     = {Association for Computing Machinery},
  address       = {New York, NY, USA},
  volume        = {27},
  number        = {4},
  issn          = {0163-5948},
  url           = {https://doi.org/10.1145/566171.566186},
  doi           = {10.1145/566171.566186},
  abstract      = {Evaluation of Code Coverage is the problem of identifying the parts of a program that did not execute in one or more runs of a program. The traditional approach for code coverage tools is to use static code instrumentation. In this paper we present a new approach to dynamically insert and remove instrumentation code to reduce the runtime overhead of code coverage. We also explore the use of dominator tree information to reduce the number of instrumentation points needed. Our experiments show that our approach reduces runtime overhead by 38-90% compared with purecov, a commercial code coverage tool. Our tool is fully automated and available for download from the Internet.},
  journal       = {SIGSOFT Softw. Eng. Notes},
  month         = {jul},
  pages         = {86–96},
  numpages      = {11},
  keywords      = {dynamic code patching, testing, code coverage, on-demand instrumentation, dominator tree, dynamic code deletion}
  }
  

@article{WDC10,
  title         = {A family of code coverage-based heuristics for effective fault localization},
  journal       = {Journal of Systems and Software},
  volume        = {83},
  number        = {2},
  pages         = {188-208},
  year          = {2010},
  note          = {Computer Software and Applications},
  issn          = {0164-1212},
  doi           = {https://doi.org/10.1016/j.jss.2009.09.037},
  url           = {https://www.sciencedirect.com/science/article/pii/S0164121209002465},
  author        = {W. {Eric Wong} and Vidroha Debroy and Byoungju Choi},
  keywords      = {Fault localization, Program debugging, Code coverage, Heuristics, Suspiciousness of code, Successful tests, Failed tests},
  abstract      = {Locating faults in a program can be very time-consuming and arduous, and therefore, there is an increased demand for automated techniques that can assist in the fault localization process. In this paper a code coverage-based method with a family of heuristics is proposed in order to prioritize suspicious code according to its likelihood of containing program bugs. Highly suspicious code (i.e., code that is more likely to contain a bug) should be examined before code that is relatively less suspicious; and in this manner programmers can identify and repair faulty code more efficiently and effectively. We also address two important issues: first, how can each additional failed test case aid in locating program faults; and second, how can each additional successful test case help in locating program faults. We propose that with respect to a piece of code, the contribution of the first failed test case that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second failed test case that executes it, which in turn is larger than or equal to that of the third failed test case that executes it, and so on. This principle is also applied to the contribution provided by successful test cases that execute the piece of code. A tool, χDebug, was implemented to automate the computation of the suspiciousness of the code and the subsequent prioritization of suspicious code for locating program faults. To validate our method case studies were performed on six sets of programs: Siemens suite, Unix suite, space, grep, gzip, and make. Data collected from the studies are supportive of the above claim and also suggest Heuristics III(a), (b) and (c) of our method can effectively reduce the effort spent on fault localization.}
}


@inproceedings{Oet+12,
  author        = {Oetsch, Johannes and Prischink, Michael and P\"{u}hrer, J\"{o}rg and Schwengerer, Martin and Tompits, Hans},
  title         = {On the Small-Scope Hypothesis for Testing Answer-Set Programs},
  year          = {2012},
  isbn          = {9781577355601},
  publisher     = {AAAI Press},
  abstract      = {In software testing, the small-scope hypothesis states that a high proportion of errors can be found by testing a program for all test inputs within some small scope. In this paper, we evaluate the small-scope hypothesis for answer-set programming (ASP). To this end, we follow work in traditional testing and base our evaluation on mutation analysis. In fact, we show that a rather limited scope is sufficient for testing ASP encodings from a representative set of benchmark problems. Our experimental evaluation facilitates effective methods for testing in ASP. Also, it gives some justification to analyse programs at the propositional level after grounding them over a small domain.},
  booktitle     = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
  pages         = {43–53},
  numpages      = {11},
  location      = {Rome, Italy},
  series        = {KR'12}
}


@InProceedings{GOT17,
  author        = {Gre{\ss}ler, Alexander and Oetsch, Johannes and Tompits, Hans},
  editor        = {Balduccini, Marcello and Janhunen, Tomi},
  title         = {{\$}{\$}{\backslash}mathsf {\{}Harvey{\}}{\$}{\$}: A System for Random Testing in ASP},
  booktitle     = {Logic Programming and Nonmonotonic Reasoning},
  year          = {2017},
  publisher     = {Springer International Publishing},
  address       = {Cham},
  pages         = {229--235},
  abstract      = {We present {\$}{\$}{\backslash}mathsf {\{}Harvey{\}}{\$}{\$}, a tool for random testing in answer-set programming (ASP) that allows to incorporate constraints to guide the generation of test inputs. Due to the declarative nature of ASP, it can be argued that there is less need for testing than in conventional software development. However, it is shown in practice that testing is still needed when more sophisticated methods are not viable. Random testing is recognised as a simple yet effective method in this regard. The approach described in this paper allows for random testing of answer-set programs in which both test-input generation and determining test verdicts is facilitated using ASP itself: The test-input space is defined using ASP rules and uniformity of test-input selection is achieved by using XOR sampling. This allows to go beyond simple random testing by adding further ASP constraints in the process.},
  isbn          = {978-3-319-61660-5},
  doi           = {10.1007/978-3-319-61660-5_21}
}


@InProceedings{ABR21,
  author        = {Amendola, Giovanni and Berei, Tobias and Ricca, Francesco},
  editor        = {Faber, Wolfgang and Friedrich, Gerhard and Gebser, Martin and Morak, Michael},
  title         = {Testing in ASP: Revisited Language and Programming Environment},
  booktitle     = {Logics in Artificial Intelligence},
  year          = {2021},
  publisher     = {Springer International Publishing},
  address       = {Cham},
  pages         = {362--376},
  abstract      = {Unit testing frameworks are nowadays considered a best practice, foregone in almost all modern software development processes, to achieve rapid development of correct specifications. The first unit testing specification language for Answer Set Programming (ASP) was proposed in 2011 as a feature of the ASPIDE development environment. Later, a more portable unit testing language was included in the LANA annotation language. In this paper we propose a revisited unit testing specification language that allows one to inline tests within ASP program and an ASP-based test execution mechanism. Moreover, we present a programming environment supporting test driven development (TDD) of ASP programs with our language.},
  isbn          = {978-3-030-75775-5},
  doi           = {10.1007/978-3-030-75775-5_24}
}


@article{DBLP:journals/stvr/BelliJ98,
  author    = {Fevzi Belli and Oliver Jack},
  title     = {Declarative Paradigm of Test Coverage},
  journal   = {Softw. Test. Verification Reliab.},
  volume    = {8},
  number    = {1},
  pages     = {15--47},
  year      = {1998},
  url       = {https://doi.org/10.1002/(SICI)1099-1689(199803)8:1\<15::AID-STVR146\>3.0.CO;2-D},
  doi       = {10.1002/(SICI)1099-1689(199803)8:1\<15::AID-STVR146\>3.0.CO;2-D},
  timestamp = {Wed, 01 Apr 2020 08:46:21 +0200},
  biburl    = {https://dblp.org/rec/journals/stvr/BelliJ98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}